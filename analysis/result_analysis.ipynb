{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reproduce the analysis in the paper\n",
    "\n",
    "This notebook describes the steps necessary to analyze the experimental results and get the plots in the paper.\n",
    "_To cut run time, the pre-computed results are in a dedicated folder: `src/utils.RESULTS_DIR`. The corresponding cells are commented out._"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import src.utils as u\n",
    "import src.rank_metrics as rm\n",
    "import src.rank_utils as ru\n",
    "\n",
    "if Path(os.getcwd()).name != \"EncoderBenchmarking\":\n",
    "    os.chdir(\"..\")  # move kernel to EncoderBenchmarking"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Results loading\n",
    "\n",
    "Load the results and the precomputed rankings, stored in the `src/utils.RESULTS_DIR` directory (default: `experimental_results`)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_parquet(u.RESULTS_DIR / \"results.parquet\")\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the rankings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = ru.get_rankings(df, factors=[\"dataset\", \"model\", \"tuning\", \"scoring\"], alternatives=\"encoder\", target=\"cv_score\",\n",
    "                     increasing=True, impute_missing=False)\n",
    "rf.to_parquet(u.RESULTS_DIR / \"rankings.parquet\")\n",
    "rf.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sensitivity analysis\n",
    "The relative performance of encoders can depend on an ML model, a quality metric, or a tuning strategy. The choice of an aggregation strategy impacts the consensus ranking.\n",
    "To quantify the influence of these choices, we calculate the similarity between rankings using the Jaccard index $J$ for the sets of best encoders as well as the Spearman correlation coefficient $\\rho$.\n",
    "For both measures, values close to 1 indicate high agreement between rankings and low sensitivity. Conversely, values near 0 (or, for $\\rho$, negative) suggest low consistency and high sensitivity.\n",
    "\n",
    "### Sensitivity to experimental factors\n",
    "We evaluate the sensitivity of encoder rankings on individual datasets with respect to an experimental factor (ML model, quality metric, or tuning strategy) by varying the factor of interest and keeping the other factors fixed, then calculating the similarity between pairs of rankings. After that, we average the result across all combinations of the other factors.\n",
    "Our findings highlight the high sensitivity of results of studies comparing encoders to experimental factors, for both the full rankings and the best encoders.\n",
    "They also explain why results from other studies are not comparable, as choosing different values for any factor will lead to different results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "jaccard_model, rho_model = u.pairwise_similarity_wide_format(rf, simfuncs=[rm.jaccard_best, rm.spearman_rho], shared_levels=[0, 2, 3])\n",
    "jaccard_tuning, rho_tuning = u.pairwise_similarity_wide_format(rf, simfuncs=[rm.jaccard_best, rm.spearman_rho], shared_levels=[0, 1, 3])\n",
    "jaccard_scoring, rho_scoring = u.pairwise_similarity_wide_format(rf, simfuncs=[rm.jaccard_best, rm.spearman_rho], shared_levels=[0, 1, 2])\n",
    "\n",
    "jaccard = reduce(lambda x, y: x.fillna(y), [jaccard_model, jaccard_tuning, jaccard_scoring])\n",
    "rho = reduce(lambda x, y: x.fillna(y), [rho_model, rho_tuning, rho_scoring])\n",
    "\n",
    "jaccard.to_parquet(u.ANALYSIS_DIR / \"pw_jaccard.parquet\")\n",
    "rho.to_parquet(u.ANALYSIS_DIR / \"pw_rho.parquet\")\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the sensitivity of results to experimental factors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sims = u.load_similarity_dataframes()\n",
    "rho = sims[\"pw_rho\"]\n",
    "jaccard = sims[\"pw_jaccard\"]\n",
    "\n",
    "factors = [\"model\", \"tuning\", \"scoring\"]\n",
    "similarities = [\"rho\", \"jaccard\"]\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "sns.set_style(\"ticks\")\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = r'\\usepackage{mathptmx}'\n",
    "mpl.rc('font', family='Times New Roman')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(factors), figsize=(5, 3))\n",
    "for ax, factor in zip(axes, factors):\n",
    "    df_sim = u.join_wide2long({\"jaccard\": jaccard, \"rho\": rho}, comparison_level=factor)\n",
    "    title = factor\n",
    "    u.heatmap_longformat_multisim(df_sim, similarities, factor, fontsize=8, annot_fontsize=8, ax=ax)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout(pad=0.5)\n",
    "fig.savefig(u.FIGURES_DIR / \"heatmap_model_tuning_scoring.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sensitivity to aggregation strategy\n",
    "\n",
    "To evaluate the impact of the aggregation strategy on the consensus ranking, we apply the same procedure as above to consensus rankings instead of rankings on individual datasets.\n",
    "For example, Spearman's $\\rho$ between consensus rankings obtained with Q-M and Q-Md averaged across all ML models, tuning strategies, and quality metrics is 0.8.\n",
    "\n",
    "While some aggregation strategies show strong similarities, different strategies yield very different consensus rankings in general.\n",
    "This is particularly evident for Jaccard index $J$, indicating the high sensitivity of the best encoders to the rank aggregation strategy.\n",
    "\n",
    "We begin by computing the consensus rankings with different aggregation strategies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%time\n",
    "# consensuses = list()\n",
    "#\n",
    "# # --- Aggregation for all aggregation strategies except Nemenyi\n",
    "# consensuses.append(ru.Aggregator(df, rf).aggregate(verbose=False, ignore_strategies=[\"nemenyi rank\"]).aggrf)\n",
    "#\n",
    "# # --- Aggregation with Nemenyi with different significance levels\n",
    "# for alpha in tqdm([0.01, 0.05, 0.1]):\n",
    "#     agg = ru.Aggregator(df, rf)\n",
    "#     bag = agg.base_aggregators[('SVC', 'full', 'AUC')]\n",
    "#     consensuses.append(agg.aggregate(verbose=False, strategies=[\"nemenyi rank\"], alpha=alpha).aggrf)\n",
    "#\n",
    "# aggrf = pd.concat(consensuses, axis=1)\n",
    "# aggrf.to_parquet(u.ANALYSIS_DIR / \"consensuses.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then compute the similarity between different consensuses ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aggrf = u.load_aggrf().rename(columns=u.AGGREGATION_NAMES, level=\"aggregation\")\n",
    "agg_jaccard, agg_rho = u.pairwise_similarity_wide_format(aggrf, simfuncs=[rm.jaccard_best, rm.spearman_rho], shared_levels=slice(-1))\n",
    "\n",
    "agg_jaccard.to_parquet(u.ANALYSIS_DIR / \"pw_AGG_jaccard.parquet\")\n",
    "agg_rho.to_parquet(u.ANALYSIS_DIR / \"pw_AGG_rho.parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "... and finally plot it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "aggsims = u.load_aggregated_similarity_dataframes()\n",
    "agg_jaccard = aggsims[\"pw_AGG_jaccard\"]\n",
    "agg_rho = aggsims[\"pw_AGG_rho\"]\n",
    "\n",
    "df_sim = u.join_wide2long({\"rho\": agg_rho, \"jaccard\": agg_jaccard}, comparison_level=\"aggregation\")\n",
    "\n",
    "sns.set_style(\"ticks\")\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = r'\\usepackage{mathptmx}'\n",
    "mpl.rc('font', family='Times New Roman')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3.3))\n",
    "u.heatmap_longformat_multisim(df_sim, [\"rho\", \"jaccard\"], \"aggregation\", fontsize=7, annot_fontsize=7, ax=ax)\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(u.FIGURES_DIR / f\"heatmap_aggregation.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replicability\n",
    "\n",
    "Replicability is the property of a benchmark of obtaining consistent results from different data. We quantify replicability by comparing consensus rankings aggregated on two disjoint samples of datasets of equal size.\n",
    "Each data point represents 100 iterations of randomly selecting two disjoint sets of datasets and applying multiple aggregation strategies. We exclude R-Kem due to its high computational complexity.\n",
    "Even with 25 datasets, replicability is moderate: this suggests that consensus rankings vary significantly depending on the data.\n",
    "Results from logistic regression tend to exhibit the highest replicability among ML models, while decision trees show the lowest.\n",
    "\n",
    "The next cells perform the replicability analysis and plot the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "df = pd.read_parquet(u.RESULTS_DIR / \"results.parquet\")\n",
    "rf = pd.read_parquet(u.RESULTS_DIR / \"rankings.parquet\")\n",
    "\n",
    "for tuning in [\"no\", \"model\", \"full\"]:\n",
    "    a = ru.replicability_analysis(df, rf, tuning, sample_sizes=[5, 10, 15, 20, 25], repetitions=100, seed=1444, append_to_existing=False, save=True, ignore_strategies=[\"nemenyi rank\", \"kemeny rank\"])\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_df_sim = pd.concat([pd.read_parquet(u.ANALYSIS_DIR / f\"sample_sim_{tuning}.parquet\") for tuning in (\"no\", \"model\", \"full\")], axis=0)\n",
    "sample_df_sim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now plot the replicability conditional on either model, scoring, or aggregation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(u)\n",
    "\n",
    "\n",
    "factor = \"scoring\"  # choices: model, scoring, aggregation\n",
    "fig = u.lineplot_replicability(sample_df_sim, hue=factor, show=True)\n",
    "\n",
    "fig.savefig(u.FIGURES_DIR / f\"lineplot_replicability_{factor}.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparing encoders\n",
    "We now examine the ranks of encoders limited to decision trees, logistic regression, and all ML models.\n",
    "\n",
    "The left subplot shows the rank of encoders from the experiments with decision trees across all datasets, quality metrics, and tuning strategies.\n",
    "One-Hot is the best-performing encoder; however, Nemenyi tests at a significance level of 0.05 fail to reject that the average rank of One-Hot is the same as that of the other encoders.\n",
    "\n",
    "The central subplot features the encoder ranks for logistic regression, where four encoders, namely One-Hot, Sum, Binary, and Weight of Evidence, consistently achieve higher ranks compared to the others.\n",
    "Nemenyi tests confirm that this difference in ranks is significant.\n",
    "These results indicate low replicability of the results for decision trees and higher replicability for logistic regression.\n",
    "\n",
    "The right plot presents the ranks of encoders across all datasets, ML models, quality metrics, and tuning strategies.\n",
    "Similarly to logistic regression, One-Hot, Sum, Binary, and Weight of Evidence consistently achieve significantly higher average ranks compared to the other encoders, again confirmed by Nemenyi tests.\n",
    "We recommend these four encoders as the preferred choices in practical applications.\n",
    "This conclusion contradicts other studies reporting a suboptimal performance of One-Hot~\\cite{cerda_similarity_2018, pargent_regularized_2022}.\n",
    "\n",
    "Our findings also reveal that Drop performs significantly worse than all other encoders, i.e., encoding categorical attributes generally yields better results than dropping them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = pd.read_parquet(u.RESULTS_DIR / \"rankings.parquet\")\n",
    "models = [None, \"DTC\", \"LR\", \"KNC\", \"LGBMC\", \"SVC\"]\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "sns.set_style(\"ticks\", {\"ytick.left\": False})\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = r'\\usepackage{mathptmx}'\n",
    "mpl.rc('font', family='Times New Roman')\n",
    "\n",
    "for model in models:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(1.8, 4.4))\n",
    "    u.boxplot_encoder_ranks(rf, ax, model)\n",
    "    if model is None:\n",
    "        ax.set_title(\"all models\")\n",
    "    else:\n",
    "        ax.set_title(u.FACTOR_LATEX[\"model\"][model])\n",
    "    ax.set_xlabel(\"rank\")\n",
    "    sns.despine(left=True, trim=True)\n",
    "    plt.tight_layout(w_pad=0.5)\n",
    "\n",
    "    fig.savefig(u.FIGURES_DIR / f\"boxplot_rank_{model}.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next cell produces the plot for the GitHub repo."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = pd.read_parquet(u.RESULTS_DIR / \"rankings.parquet\")\n",
    "models = [None, \"DTC\", \"LR\"]\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "sns.set_style(\"ticks\", {\"ytick.left\": False})\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = r'\\usepackage{mathptmx}'\n",
    "mpl.rc('font', family='Times New Roman')\n",
    "\n",
    "fig, axes = plt.subplots(1, len(models))\n",
    "for ax, model in zip(axes.flatten(), models):\n",
    "    u.boxplot_encoder_ranks(rf, ax, model)\n",
    "    if model is None:\n",
    "        ax.set_title(\"all models\")\n",
    "    else:\n",
    "        ax.set_title(u.FACTOR_LATEX[\"model\"][model])\n",
    "    ax.set_xlabel(\"rank\")\n",
    "sns.despine(left=True, trim=True)\n",
    "plt.tight_layout(w_pad=0.5)\n",
    "\n",
    "fig.savefig(u.FIGURES_DIR / \"encoder_ranks.png\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Missing evaluations\n",
    "\n",
    "The following cell computes the number of missing evaluations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import src.config as cfg\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "total_runs = {}\n",
    "for tuning_strategy in [\"no tuning\", \"model tuning\", \"full tuning\"]:\n",
    "    total_runs[tuning_strategy] = product(cfg.ENCODERS,\n",
    "                                          cfg.DATASET_IDS[tuning_strategy],\n",
    "                                          cfg.MODELS[tuning_strategy],\n",
    "                                          cfg.SCORINGS)\n",
    "num_runs = sum(len(list(x)) for x in total_runs.values())\n",
    "\n",
    "print(f\"Total runs     : {num_runs}\")\n",
    "print(f\"Completed runs : {len(df)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Runtime\n",
    "\n",
    "The following cells studies the runtime of encoders."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_parquet(u.RESULTS_DIR / \"results.parquet\")\n",
    "df_ = df.copy()\n",
    "df_[\"encoder\"] = df_[\"encoder\"].map(u.ENCODER_LATEX)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2))\n",
    "ax = u.boxplot_runtime(df_.query(\"tuning == 'full'\"), ax=ax)\n",
    "sns.despine(bottom=True, trim=True)\n",
    "plt.tight_layout(w_pad=0.5)\n",
    "fig.savefig(u.FIGURES_DIR / \"boxplot_tuning_time.pdf\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 2))\n",
    "ax = u.boxplot_runtime(df_.query(\"tuning != 'full'\"), ax=ax)\n",
    "sns.despine(bottom=True, trim=True)\n",
    "plt.tight_layout(w_pad=0.5)\n",
    "fig.savefig(u.FIGURES_DIR / \"boxplot_encoding_time.pdf\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning\n",
    "\n",
    "This cell investigates if tuning lead to an increase in pipeline performance wrt. no tuning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_parquet(u.RESULTS_DIR / \"results.parquet\")\n",
    "df_ = u.get_dataset_tuning_comparison(df)\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "sns.set_style(\"ticks\")\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = r'\\usepackage{mathptmx}'\n",
    "mpl.rc('font', family='Times New Roman')\n",
    "\n",
    "factor = \"scoring\"\n",
    "df_[factor] = df_[factor].map(u.FACTOR_LATEX[factor])\n",
    "fig, axes = plt.subplots(1, 2, figsize=(2.5, 2), sharey=\"all\")\n",
    "for it, ((tuning_x, tuning_y), ax) in enumerate(zip([(\"full\", \"no\"), (\"full\", \"model\")], axes)):\n",
    "    ax = sns.boxplot(data=df_.query(\"tuning_x == @tuning_x and tuning_y == @tuning_y\"), x=factor, y=\"gain\",\n",
    "                                 # palette=sns.light_palette(\"grey\", n_colors=len(rf.index)),\n",
    "                                 color=\"lightgrey\",\n",
    "                                 showfliers=False,\n",
    "                                 linewidth=1, showcaps=False,\n",
    "                                 showmeans=True,\n",
    "                                 meanprops={\"marker\": \"o\",\n",
    "                                            \"markeredgecolor\": \"red\",\n",
    "                                            \"markersize\": 2},\n",
    "                                 medianprops={\"linestyle\": \"-\"\n",
    "                                              },\n",
    "                                 ax=ax)\n",
    "    ax.grid(axis=\"y\", zorder=-1, linewidth=0.4)\n",
    "\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=90,\n",
    "    )\n",
    "\n",
    "    if it != 0:\n",
    "        ax.tick_params(left=False)\n",
    "\n",
    "    ax.set_title(f\"{tuning_x} VS {tuning_y}\")\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout(pad=0.5)\n",
    "\n",
    "plt.savefig(u.FIGURES_DIR / f\"boxplot_tuningeffect_{factor}.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_parquet(u.RESULTS_DIR / \"results.parquet\")\n",
    "df_ = u.get_dataset_tuning_comparison(df)\n",
    "\n",
    "sns.set(font_scale=0.8)\n",
    "sns.set_style(\"ticks\")\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "mpl.rcParams['text.latex.preamble'] = r'\\usepackage{mathptmx}'\n",
    "mpl.rc('font', family='Times New Roman')\n",
    "\n",
    "df_[\"encoder\"] = df_[\"encoder\"].map(u.ENCODER_LATEX)\n",
    "for tuning_x, tuning_y in [(\"full\", \"no\"), (\"full\", \"model\")]:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 2))\n",
    "    ax = u.sorted_boxplot_vertical(data=df_.query(\"tuning_x == @tuning_x and tuning_y == @tuning_y\"), x=\"encoder\", y=\"gain\", order_by=\"mean\",\n",
    "                                 # palette=sns.light_palette(\"grey\", n_colors=len(rf.index)),\n",
    "                                 color=\"lightgrey\",\n",
    "                                 showfliers=False,\n",
    "                                 linewidth=1, showcaps=False,\n",
    "                                 showmeans=True,\n",
    "                                 meanprops={\"marker\": \"o\",\n",
    "                                            \"markeredgecolor\": \"red\",\n",
    "                                            \"markersize\": 2},\n",
    "                                 medianprops={\"linestyle\": \"-\"\n",
    "                                              },\n",
    "                                 ax=ax)\n",
    "    ax.grid(axis=\"y\", zorder=-1, linewidth=0.4)\n",
    "\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_xticklabels(\n",
    "        ax.get_xticklabels(),\n",
    "        rotation=90,\n",
    "    )\n",
    "\n",
    "    ax.set_ylim([-0.025, 0.050])\n",
    "\n",
    "    # ax.set_title(f\"{tuning_x} VS {tuning_y}\")\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout(pad=0.5)\n",
    "\n",
    "    plt.savefig(u.FIGURES_DIR / f\"boxplot_tuningeffect_encoder_{tuning_x}_{tuning_y}.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}